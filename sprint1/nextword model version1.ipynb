{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2f0072-4cca-4192-b2c8-0a1c4b2d97e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajai5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Function to generate text\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mTextDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     37\u001b[0m     batch_passages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassages[index \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:(index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m---> 38\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_passages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mTextDataGenerator.generate_sequences\u001b[1;34m(self, passages)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list)):\n\u001b[0;32m     49\u001b[0m             n_gram_sequence \u001b[38;5;241m=\u001b[39m token_list[:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m             \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_gram_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m             y\u001b[38;5;241m.\u001b[39mappend(n_gram_sequence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     52\u001b[0m X \u001b[38;5;241m=\u001b[39m pad_sequences(X, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Load the preprocessed text data\n",
    "df = pd.read_csv('cleaned_paragraphs.csv')\n",
    "passages = df['Paragraphs'].tolist()  # Ensure column name matches the CSV file\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(passages)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the sequence length and batch size\n",
    "sequence_length = 50\n",
    "batch_size = 64\n",
    "\n",
    "class TextDataGenerator(Sequence):\n",
    "    def __init__(self, passages, tokenizer, sequence_length, batch_size):\n",
    "        self.passages = passages\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.passages) * self.sequence_length / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_passages = self.passages[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.generate_sequences(batch_passages)\n",
    "        return X, y\n",
    "\n",
    "    def generate_sequences(self, passages):\n",
    "        X = []\n",
    "        y = []\n",
    "        for passage in passages:\n",
    "            sequences = self.create_sequences(passage)\n",
    "            for sequence in sequences:\n",
    "                token_list = self.tokenizer.texts_to_sequences([sequence])[0]\n",
    "                for i in range(1, len(token_list)):\n",
    "                    n_gram_sequence = token_list[:i+1]\n",
    "                    X.append(n_gram_sequence[:-1])\n",
    "                    y.append(n_gram_sequence[-1])\n",
    "        X = pad_sequences(X, maxlen=self.sequence_length, padding='pre')\n",
    "        y = to_categorical(y, num_classes=self.total_words)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def create_sequences(self, text):\n",
    "        words = text.split()\n",
    "        sequences = []\n",
    "        for i in range(len(words) - self.sequence_length):\n",
    "            seq = words[i:i+self.sequence_length+1]\n",
    "            sequences.append(' '.join(seq))\n",
    "        return sequences\n",
    "\n",
    "# Split the data\n",
    "train_passages, val_passages = train_test_split(passages, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = TextDataGenerator(train_passages, tokenizer, sequence_length, batch_size)\n",
    "val_generator = TextDataGenerator(val_passages, tokenizer, sequence_length, batch_size)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=sequence_length-1),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define model checkpoint callback with .keras extension\n",
    "checkpoint = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_len+1:] if len(token_list) > max_sequence_len else token_list\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted, axis=-1)[0]\n",
    "\n",
    "        # Ensure the predicted index is valid\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Handle case where no valid word is found\n",
    "        if not output_word:\n",
    "            break\n",
    "        \n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# Generate text\n",
    "print(generate_text(\"Your seed text here\", 50, model, tokenizer, sequence_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312a64-71d8-48c1-9355-68a35ca54a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
